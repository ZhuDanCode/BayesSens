---
title: "`BayesSense` Gaussian distribution with independent Normal-Inverse Gamma priors"
# author: "[add author name]"
date: "Last updated on: `r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width=7, fig.height=5,
  eval = T
)
```

In this vignette, we introduce some basic functionalities of the `BayesSense` package to do Bayesian sensitivity analysis of the Gaussian distribution with independent Normal-Inverse Gamma priors. Similar analysis can be performed on other models in the package, i.e. the Student-t model and the 2-equation SUR model.

## Content {#content}

1. [Data simulation](#data_sim)
2. [Model specification](#model_spec)
3. [MCMC Inference](#mcmc_infer)
4. [MCMC Sensitivity Analysis](#mcmc_sense)
5. [Reference](#reference)

## 1. Data simulation {#data_sim}
To begin with, we load the package and simulate some data from the gaussian model with the function `gaussian_data(n, p)`, where `n` is the number of datapoints, and `p` is the number of measurements per datapoint. There are two optional arguments `beta` and `sigma`, corresponding to the regression coefficients and the variance in the classical linear regression model.

```{r}
library(BayesSense)
set.seed(1234)  # For reproducibility 
n <- 500
p <- 3
data0 <- gaussian_data(n, p, intercept = T)
str(data0)

# Alternatively, you can supply your own coefficients
beta0 <- c(-0.1, 0.1, -0.1, 0.1)
sigma0 <- 10
data0 <- gaussian_data(n, p, beta = beta0, sigma = sigma0, intercept = T)
str(data0)
```


## 2. Model specification {#model_spec}

Denote the response variable by $y$ ($n$ vector), the predictor variable by $X$ ($n \times p$ matrix), the *Gaussian model with independent Normal-Inverse Gamma priors* are given as follows:
$$y \sim N\left(X\beta, \sigma^2 I_p\right)$$

with prior distributions
$$\beta \sim N_p\left(\beta_0, B_0\right) \quad \text{and}\quad \sigma^2 \sim IG\left(\dfrac{\alpha_0} 2, \dfrac{\delta_0} 2 \right)$$


## 3. MCMC inference {#mcmc_infer}
We fit the model using MCMC with the function `gaussian_Gibbs`. We need to provide `b_0, B_0, alpha_0` and `delta_0`. 

```{r, results='hide'}
res <- gaussian_Gibbs(
  X = data0$X, y = data0$y,
  b_0 = numeric(p+1), B_0 = diag(p+1), # add one for intercept
  alpha_0 = 13, delta_0 = 8, 
  num_steps = 1000
)
```

The result contains the 1000 posterior samples of the parameters. 
```{r}
str(res)
# Note that the number of MCMC iterations is set to be 1000 by default, so we have 1000 posterior samples for each parameter. 
```

We can inspect the posterior distribution of the parameters individually with `hist`, `density` or `plot_posterior`

```{r}
# Single plot
hist(res$beta[,2], main = "Posterior distribution", prob = T)
lines(density(res$beta[,2]), col = "blue", lwd = 2)
abline(v = data0$beta[2], col = "red", lwd = 3)    # Actual coefficient
# Multiple plots
plot_posterior(res)
```


## 4. MCMC Sensitivity Analysis {#mcmc_sense}
We conduct the sensitivity analysis using the `gaussian_AD` function. This function returns *both* the posterior samples and the sensitivity of posterior sample (at each iteration) w.r.t. the prior parameters. In general, one does not need to run `gaussian_Gibbs` before `gaussian_AD`, unless one wants only the posterior samples but not their sensitivities. 

As a sidenote, this function has the same interface as `gaussian_Gibbs`.


```{r, results='hide'}
res <- gaussian_AD(
  X = data0$X, y = data0$y,
  b_0 = numeric(p+1), B_0 = diag(p+1),   # add one for intercept
  alpha_0 = 13, delta_0 = 8,
  num_steps = 1000
)
```

```{r}
str(res)
```

One can directly manipulate the sensitivity results (using `$`), or use the helper functions `available_sensitivity` and `get_sensitivity` to retrieve the sensitivities of interest. Here we consider $\dfrac{d \beta}{dB_0}$ as an example. 

```{r}
available_sensitivity(res)

d_beta_on_d_B0 <- get_sensitivity(res, "d_beta", "d_B0")
str(d_beta_on_d_B0)  # dimension = mcmc steps x length(beta) x length(B_0)
```

Using the matrix calculus notation in [Magnus and Neudecker (1988)](#reference), $\dfrac{d \beta}{dB_0}$ is understood as $\dfrac{d \,vec(\beta)}{d\,vec(B_0)^T}$, where $vec$ denotes the vectorisation operator. 
Since $\beta$ has $4$ elements, and $B_0$ have $16$ elements, $\dfrac{d \beta}{dB_0}$ is a $4 \times 16$ matrix, with the $(i,j)$ term representing the sensitivity 
$$\dfrac{d \beta^{i}}{dB_0^{j}} \quad i = 1, 2, 3, 4,\;\; j = 1, 2, ..., 16.$$
We add an index for the $g$-th iteration to denote the sensitivity of the posterior sample $\beta^{i}$ at iteration $g$ w.r.t. $B_0^{j}$:
$$\dfrac{d \beta^{i}_{(g)}}{dB_0^{j}} \quad i = 1, 2, 3, 4,\;\; j = 1, 2, ..., 16, \;\; g = 1, 2, ..., `r nrow(d_beta_on_d_B0)`$$.

### Prior robustness
Prior robustness is a measure of the sensitivity of the posterior distribution with respect to the prior specification. In particular, we investigate how much some functional of the posterior distribution changes as the prior parameters change locally by examining the corresponding Jacabian matrix.
```{r}
jacobian(d_beta_on_d_B0, stat_fun = mean)
```

Often, we want to examine the sensitivity over iterations. Continuing with our example, we want to plot the posterior mean, $d\left(\displaystyle\dfrac 1 n \sum_{g=1}^n \beta^1_{(g)}\right) \bigg /dB_0^5$ against $n$, where $\beta^1_{(g)}$ is the posterior sample of $\beta^1$ at iteration $g$. We first introduce the `cumulative_stat` function which has the following behaviour.

```{r}
mean(1:10)
cumulative_stat(1:10, mean)

sd(1:10)
cumulative_stat(1:10, sd)
```

So if we want to see the trajectory of the posterior mean over the iterations, we can pass in `cumulative_stat` with parameter `mean` instead of just `mean` alone. Here we only show the sensitivity for the $(1,1)$ entry of the Jacobian matrix. 
```{r}
csens <- jacobian(d_beta_on_d_B0, stat_fun = cumulative_stat, fun = mean)
plot(csens[, 1, 1], type = "l", 
     main = "Sensitivity of the Posterior Mean",
     xlab = "Iterations", ylab = "Sensitivity")
```
In most applications, the resulting Jacobian is a large matrix, and it is helpful to inspect the average-case entry and the worst-case entry. Specifically, we plot the values of the average of the absolute entries and the largest absolute entry in the Jacobian against the number of iteration. 

```{r}
avg_case <- apply(abs(csens), 1, mean)
wst_case <- apply(abs(csens), 1, max)

# Setup empty plot
plot(x = range(1:length(avg_case)),
     y = extendrange(c(avg_case, wst_case), f = 0.1), 
     type = 'n', 
     main = "Summary sensitivity of the Posterior Mean",
     xlab = "Iterations", ylab = "Sensitivity")
# Add lines
lines(avg_case, lwd = 2)
lines(wst_case, col = 'red', lwd = 2)
# Add legend
legend("bottomright", col = c('black', 'red'), lwd = c(2, 2),
       legend = c("Average-absolute sensitivity", 
                  "Maximum-absolute sensitivity"))
```

### Convergence analysis

The sensitivity measure provides an interesting alternative for convergence diagnostic. As one would expect, when the posterior samples reach stationarity, they essentially "forget" where they started, i.e. they are insensitive to the starting values. In the following, we plot the sensitivity of the posterior samples of $\beta$ w.r.t. the starting value $\sigma^2_0$.

```{r, fig.height=8}
d_beta_on_d_sigma2 <- get_sensitivity(res, "d_beta", "d_sigma2_0")
sens <- jacobian(d_beta_on_d_sigma2, cumulative_stat, fun = mean)
dim(sens)

par(mfrow = c(2,2))  # Do a 2 x 2 subplots
for (i in 1:4) {
  plot(sens[,i,1], 
       main = "Sensitivity of posterior samples w.r.t. sigma2_0", 
       xlab = "Iteration", ylab = "Sensitivity", type = 'l')  
}
par(mfrow = c(1,1))  # Restore to default
```


### Note: Vectorisation operator $vec$
Suppose $A = \left[\begin{array}{cc} a &b \\ c &d \end{array}\right]$, then $vec(A) = \left[\begin{array}{c} a \\c \\ b \\d \end{array}\right]$.

## 5. Reference {#reference}
Magnus, J. R., & Neudecker, H. (1988). Matrix differential calculus with applications in statistics and econometrics. *Wiley series in probability and mathematical statistics.*
