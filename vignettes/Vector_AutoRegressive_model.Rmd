---
title: "Bayesian Vector Autoregressive Model (VAR)"
author: "Jackson Kwok"
date: "13/03/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The two major functions are `simulate_VAR` and `VAR_Gibbs`. The first simulates from a Vector-Auto-Regressive (VAR) model; the second infers the model parameters from data using Gibbs sampling.

First we introduce the simulation function. The model considered uses independent Normal distribution and inverse-Wishart distribution as priors. If the hyperparameters are not provided, then they will be generated randomly.
```{r}
library(BayesSense)
num_data <- 2
dim_data <- 2
lag <- 1

sim <- simulate_VAR(m = num_data, n = dim_data, p = lag)
sim
```


Since the formulation and the implementation of the VAR model use different notations, there are a few helper functions to convert between them.
```{r, results='hold'}
sim$data
m0 <- VAR_model_matrix(sim$data, 1)
print(m0)
```

```{r, results='hold'}
sim$model
vec0 <- VAR_model_to_vec(sim$model)
print(vec0)

VAR_vec_to_model_coeff(vec0, n = 2)
```


Next, we introduce the function for inference, `VAR_Gibbs`.
```{r, results='hide'}
# set.seed(345)
num_data <- 10000
dim_data <- 3
lag <- 2
converted_dim <- dim_data + dim_data^2 * lag

sim <- simulate_VAR(m = num_data, n = dim_data, p = lag,
                    B = list(s_eig_matrix(dim_data), s_eig_matrix(dim_data)))
res <- VAR_Gibbs(sim$data, lag = lag, 
        b_0 = rnorm(converted_dim), B_0 = pdmatrix(converted_dim)$Sigma, 
        v_0 = dim_data, S_0 = pdmatrix(dim_data)$Sigma, 
        init_Sigma = pdmatrix(dim_data)$Sigma, 
        num_steps = 3e4, burn_ins = 1e3)
```

```{r}
# Compare estimated parameters with the true ones
sim$model
VAR_posterior_stat(res, dim_data, fun = mean)
```


Finally, we introduce the function for sensitivity analysis, `VAR_AD`.
```{r}
num_data <- 1000
dim_data <- 3
lag <- 2
converted_dim <- dim_data + dim_data^2 * lag

sim <- simulate_VAR(m = num_data, n = dim_data, p = lag)
data0 <- sim$data

prior_cov <- pdmatrix(converted_dim)$Sigma
profile <- profvis::profvis({
    res <- VAR_AD(data0, lag = lag, 
         b_0 = rnorm(converted_dim), 
         B_0 = prior_cov, 
         v_0 = dim_data, 
         S_0 = pdmatrix(dim_data)$Sigma, 
         init_Sigma = diag(dim_data),
         num_steps = 1e2, burn_ins = 1e1)
})
profile
```

## Comparison with the Likelihood-Ratio approach
```{r}
library(purrr)
# Table 2
inv_pcov <- solve(prior_cov)
LR_estimate <- c(t(inv_pcov %*% cov(res$beta)))
AD_estimate <- colMeans(res$d_beta$d_b0)
cbind(LR_estimate, AD_estimate)

# Convergence comparison
# LR estimates
s <- seq(min(nrow(res$beta) / 10, 100), nrow(res$beta), length.out = 20)
LR_estimates <- map(
  s, ~c(t(inv_pcov %*% cov(res$beta[seq(.x), ])))
) %>%
  do.call(rbind, .)

# AD estimates
AD_estimates <- map(s, ~colMeans(res$d_beta$d_b0[seq(.x), ])) %>%
  do.call(rbind, .)

# Plots
for (i in 1:min(ncol(LR_estimates), 10)) {
  LR_est <- LR_estimates[,i]
  AD_est <- AD_estimates[,i]
  plot(x = s, y = LR_est, ylim = range(c(LR_est, AD_est)), type = 'n',
       main = "Prior Mean Robustness", xlab = "Length of chain",
       ylab = "Sensitivity")
  legend(x = "topright", legend = c("Likelihood Ratio", "Auto-differentiation"),
         col = c("black", "blue"), lwd = c(1, 2))
  lines(s, LR_est)
  lines(s, AD_est, col = "blue", lwd = 2)
  # if (readline("Continue? Y or N: ") == "N") break
}
```


## Fitted values and Forecast
```{r}
# Simulate and fit data, this part is the same as what we saw above.
num_data <- 300
dim_data <- 2
lag <- 2
converted_dim <- dim_data + dim_data^2 * lag

sim <- simulate_VAR(m = num_data, n = dim_data, p = lag)
data0 <- sim$data
res <- VAR_Gibbs(data0, lag = lag, 
          b_0 = rnorm(converted_dim), 
          B_0 = pdmatrix(converted_dim)$Sigma, 
          v_0 = dim_data, 
          S_0 = pdmatrix(dim_data)$Sigma,
          init_Sigma = diag(dim_data),
          num_steps = 5e4, burn_ins = 1e3)

# Extract fitted parameter
fitted_param <- colMeans(res$beta) %>% VAR_vec_to_model_coeff(lag)
data1 <- fitted_VAR(t(data0), b_0 = fitted_param$b_0, B = fitted_param$B)

# Plot original data with fitted values
par(mfrow = c(1, 2))
plot_mts(t(data0), xlab = "time", ylab = "index", main = "Data")
plot_mts(data1, xlab = "time", ylab = "index", main = "Fitted values")
par(mfrow = c(1, 1))

# Forecast
data2 <- predict_VAR(50, t(data0), b_0 = fitted_param$b_0, B = fitted_param$B)
plot_mts(data2, xlab = "time", ylab = "index", main = "Forecast")
```

```{r}
# Forecast with bands
posterior_density <- VAR_posterior_forecast(50, t(data0), res)
means <- summarise_forecast(posterior_density, mean)
bands <- summarise_forecast(posterior_density, quantile, probs = c(0.1, 0.9))

# Nice plots
library(ggplot2)
library(ggthemes)
src_data <- reshape2::melt(data0) %>% 
  purrr::set_names(c("series_id", "time", "value"))
plot_data <- means %>% 
  purrr::map(~t(matrix(.x))) %>% 
  reshape2::melt() %>% 
  dplyr::select(-Var1) %>% 
  purrr::set_names(c("time", "value", "series_id"))
plot_data$time <- plot_data$time + ncol(data0)
band_data <- reshape2::melt(bands) %>% 
  purrr::set_names(c("quantile", "time", "value", "series_id")) %>%
  tidyr::spread("quantile", "value")
band_data$time <- band_data$time + ncol(data0)
tmp_data <- src_data %>% 
  rbind(plot_data) %>% 
  dplyr::left_join(band_data, by = c("series_id", "time"))
names(tmp_data)[4:5] <- c("lbd", "ubd")
ggplot(tmp_data, aes(x = time, y = value, group = series_id)) + 
  geom_line() + theme_economist() +
  geom_ribbon(aes(x = time, ymin = lbd, ymax = ubd), alpha = 0.4) + 
  labs(title = "Vector-Auto-Regressive (VAR) Model", 
       x = "Time", y = "Index")
```
